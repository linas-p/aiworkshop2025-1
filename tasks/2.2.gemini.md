# Investigate the Google Gemini API

We following the instructions to set up the [Google Gemini API documentation](https://ai.google.dev/gemini-api/docs/quickstart?lang=python).

1. Prerequisites:
    * Install Python packages:
        ```bash
        pip install -q -U google-genai
        ```

2. Test the API for generating content:
    * Run the following code:
        ```python
        from google import genai

        client = genai.Client(api_key="YOUR_API_KEY")
        response = client.models.generate_content(
        model="gemini-2.0-flash", 
        contents="Explain how AI works"
        )
        print(response.text)
        ```

    * Run the structured prompts:
        ```python
        from google import genai

        prompt = """List a few popular cookie recipes in JSON format.

        Use this JSON schema:

        Recipe = {'recipe_name': str, 'ingredients': list[str]}
        Return: list[Recipe]"""

        client = genai.Client(api_key="GEMINI_API_KEY")
        response = client.models.generate_content(
            model='gemini-2.0-flash',
            contents=prompt,
        )

        # Use the response as a JSON string.
        print(response.text)
        ```

3. Test the API for captions/answers:
    * Run the following code:
        ```python
        from PIL import Image
        from google import genai

        client = genai.Client(api_key="GEMINI_API_KEY")

        image = Image.open("sample.png")
        response = client.models.generate_content(
            model="gemini-2.0-flash",
            contents=[image, "Generate the detailed description of the image"])
        print(response.text)
        ```

More documentation can be found [here](https://ai.google.dev/gemini-api/docs/vision?lang=python).

4. Test the API for transcription or describing an audio:
    * Run the following code:
        ```python
        myfile = client.files.upload(file='sample.mp3')
        prompt = 'Generate a transcript of the speech.'

        response = client.models.generate_content(
        model='gemini-2.0-flash',
        contents=[
            prompt,
            myfile]
        )

        print(response.text)
        ```

5. Test the API for image generation (need setup an a paid account):
    * Run the following code:
        ```python
        from google import genai
        from google.genai import types
        from PIL import Image
        from io import BytesIO

        client = genai.Client(api_key='GEMINI_API_KEY')

        response = client.models.generate_images(
            model='imagen-3.0-generate-002',
            prompt='Fuzzy samoyed on snow',
            config=types.GenerateImagesConfig(
                number_of_images= 4,
            )
        )
        for generated_image in response.generated_images:
        image = Image.open(BytesIO(generated_image.image.image_bytes))
        image.show()
        ```

6. Test Deep search API by searching in Google Search:
    * Run the following code:
        ```python
        from google import genai
        from google.genai.types import Tool, GenerateContentConfig, GoogleSearch

        client = genai.Client()
        model_id = "gemini-2.0-flash"

        google_search_tool = Tool(
            google_search = GoogleSearch()
        )

        response = client.models.generate_content(
            model=model_id,
            contents="When is the next total solar eclipse in the United States?",
            config=GenerateContentConfig(
                tools=[google_search_tool],
                response_modalities=["TEXT"],
            )
        )

        for each in response.candidates[0].content.parts:
            print(each.text)
        # Example response:
        # The next total solar eclipse visible in the contiguous United States will be on ...

        # To get grounding metadata as web content.
        print(response.candidates[0].grounding_metadata.search_entry_point.rendered_content)
        ```

Return [README.md](../README.md)